{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5943edf0-06e7-44e7-a8ef-c56c61647fe5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 1 - create_table_creators_scrape_wiki\n",
    "\n",
    "Este notebook realiza a ingestão das páginas da Wikipedia de criadores de conteúdo.  \n",
    "O fluxo inclui:  \n",
    "- leitura de um arquivo JSON/JSON.GZ parametrizado, com suporte a multiline;  \n",
    "- aplicação de schema fixo (`wiki_page`) e validações de qualidade (nulos, vazios, duplicados);  \n",
    "- escrita dos dados em tabela Delta gerenciada, com comentários de documentação em tabela e coluna.  \n",
    "\n",
    "O resultado é a criação/atualização da tabela **default.creators_scrape_wiki**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "080129b7-675b-4949-8678-836d05cc37ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Libraries and configuration\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "import logging, json, traceback\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "\n",
    "# Widgets (parameterization)\n",
    "\n",
    "dbutils.widgets.text(\n",
    "    \"input_path\",\n",
    "    \"/Volumes/workspace/default/gz_files/wiki_pages.json.gz\",\n",
    "    \"File path (.json or .json.gz)\"\n",
    ")\n",
    "dbutils.widgets.text(\n",
    "    \"output_table_name\",\n",
    "    \"default.creators_scrape_wiki\",\n",
    "    \"Target Delta table name\"\n",
    ")\n",
    "dbutils.widgets.dropdown(\n",
    "    \"multiline\",\n",
    "    \"false\",\n",
    "    [\"false\", \"true\"],\n",
    "    \"JSON multiline? (true/false)\"\n",
    ")\n",
    "\n",
    "input_path        = dbutils.widgets.get(\"input_path\").strip()\n",
    "output_table_name = dbutils.widgets.get(\"output_table_name\").strip()\n",
    "use_multiline     = dbutils.widgets.get(\"multiline\").lower() == \"true\"\n",
    "\n",
    "logging.info(f\"Arquivo de origem: {input_path}\")\n",
    "logging.info(f\"Tabela destino: {output_table_name}\")\n",
    "logging.info(f\"Leitura multiline: {use_multiline}\")\n",
    "\n",
    "\n",
    "# Input contract\n",
    "\n",
    "schema = StructType([StructField(\"wiki_page\", StringType(), True)])\n",
    "\n",
    "\n",
    "# Path validation\n",
    "\n",
    "try:\n",
    "    _ = dbutils.fs.head(input_path, 1024)\n",
    "except Exception as e:\n",
    "    msg = f\"ERRO: caminho inválido ou inacessível: {input_path} | {e}\"\n",
    "    logging.error(msg)\n",
    "    dbutils.notebook.exit(msg)\n",
    "\n",
    "\n",
    "# Ingestion\n",
    "\n",
    "try:\n",
    "    reader = spark.read.schema(schema)\n",
    "    if use_multiline:\n",
    "        reader = reader.option(\"multiLine\", \"true\")\n",
    "    creators_df = reader.json(input_path)\n",
    "except Exception as e:\n",
    "    logging.error(\"Falha ao ler o JSON de origem:\\n\" + traceback.format_exc())\n",
    "    dbutils.notebook.exit(f\"ERRO ao ler JSON: {e}\")\n",
    "\n",
    "\n",
    "# Cleaning and validation\n",
    "\n",
    "creators_df = (\n",
    "    creators_df\n",
    "      .select(F.trim(F.col(\"wiki_page\")).alias(\"wiki_page\"))\n",
    "      .filter(F.col(\"wiki_page\").isNotNull() & (F.col(\"wiki_page\") != \"\"))\n",
    "      .dropDuplicates([\"wiki_page\"])\n",
    ")\n",
    "\n",
    "# empty check without using .isEmpty()\n",
    "if creators_df.limit(1).count() == 0:\n",
    "    dbutils.notebook.exit(\"ERRO: nenhum registro válido após limpeza.\")\n",
    "\n",
    "\n",
    "# Write (Delta)\n",
    "\n",
    "try:\n",
    "    (creators_df.write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .option(\"overwriteSchema\", \"true\")\n",
    "        .saveAsTable(output_table_name)\n",
    "    )\n",
    "\n",
    "    spark.sql(f\"\"\"\n",
    "    COMMENT ON TABLE {output_table_name}\n",
    "    IS 'Tabela de páginas da Wikipedia (creators) — origem: {input_path}'\n",
    "    \"\"\")\n",
    "\n",
    "    spark.sql(f\"\"\"\n",
    "    COMMENT ON COLUMN {output_table_name}.wiki_page\n",
    "    IS 'Nome da página da Wikipedia associada a um criador de conteúdo'\n",
    "    \"\"\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(\"Falha ao escrever na tabela Delta:\\n\" + traceback.format_exc())\n",
    "    dbutils.notebook.exit(f\"ERRO ao escrever Delta: {e}\")\n",
    "\n",
    "\n",
    "# Return\n",
    "\n",
    "final_count = spark.table(output_table_name).count()\n",
    "payload = {\n",
    "    \"status\": \"SUCESSO\",\n",
    "    \"source_path\": input_path,\n",
    "    \"target_table\": output_table_name,\n",
    "    \"records_processed\": final_count\n",
    "}\n",
    "logging.info(f\"Processo finalizado. Registros ingeridos: {final_count}\")\n",
    "dbutils.notebook.exit(json.dumps(payload, ensure_ascii=False))\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6692143713836146,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "1 - create_table_creators_scrape_wiki",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
