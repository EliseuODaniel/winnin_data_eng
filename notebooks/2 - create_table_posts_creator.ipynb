{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e44e54d-dc4a-4c36-9fa1-1fcf0dd71b4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 2 - create_table_posts_creator\n",
    "\n",
    "Este notebook realiza a ingestão dos dados brutos de posts de criadores de conteúdo.  \n",
    "O fluxo inclui:  \n",
    "- leitura de arquivo JSON/JSON.GZ parametrizado com schema fixo (views, likes, título, data, tags, yt_user);  \n",
    "- normalização do campo `published_at` em `published_at_ts` (timestamp) e `published_month` (date);  \n",
    "- aplicação de validações de sanidade (curtidas/visualizações não negativas, remoção de duplicados, checagem de nulos);  \n",
    "- escrita dos dados em tabela Delta gerenciada, com comentários de documentação em tabela e colunas.  \n",
    "\n",
    "O resultado é a criação/atualização da tabela **default.posts_creator**.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7f014e1-60aa-4805-9ee2-7f3516c3bc1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Libraries and configuration\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, LongType, ArrayType\n",
    ")\n",
    "import logging, json, traceback\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "\n",
    "# Widgets (parameterization)\n",
    "\n",
    "dbutils.widgets.text(\n",
    "    \"input_path\",\n",
    "    \"/Volumes/workspace/default/gz_files/posts_creator.json.gz\",\n",
    "    \"File path (.json/.json.gz)\"\n",
    ")\n",
    "dbutils.widgets.text(\n",
    "    \"output_table_name\",\n",
    "    \"default.posts_creator\",\n",
    "    \"Target Delta table name\"\n",
    ")\n",
    "dbutils.widgets.dropdown(\n",
    "    \"multiline\", \"false\", \n",
    "    [\"false\", \"true\"], \n",
    "    \"JSON multiline? (true/false)\"\n",
    ")\n",
    "\n",
    "input_path        = dbutils.widgets.get(\"input_path\").strip()\n",
    "output_table_name = dbutils.widgets.get(\"output_table_name\").strip()\n",
    "use_multiline     = dbutils.widgets.get(\"multiline\").lower() == \"true\"\n",
    "\n",
    "logging.info(f\"Arquivo de origem: {input_path}\")\n",
    "logging.info(f\"Tabela destino: {output_table_name}\")\n",
    "logging.info(f\"Leitura multiline: {use_multiline}\")\n",
    "\n",
    "\n",
    "# Data contract\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"creator_id\",  StringType(), True),\n",
    "    StructField(\"views\",       LongType(),   True),\n",
    "    StructField(\"likes\",       LongType(),   True),\n",
    "    StructField(\"title\",       StringType(), True),\n",
    "    StructField(\"published_at\",StringType(), True),  # will be normalized\n",
    "    StructField(\"tags\",        ArrayType(StringType()), True),\n",
    "    StructField(\"yt_user\",     StringType(), True),\n",
    "])\n",
    "\n",
    "\n",
    "# Path validation\n",
    "\n",
    "try:\n",
    "    _ = dbutils.fs.head(input_path, 1024)\n",
    "except Exception as e:\n",
    "    msg = f\"ERRO: caminho inválido ou inacessível: {input_path} | {e}\"\n",
    "    logging.error(msg)\n",
    "    dbutils.notebook.exit(msg)\n",
    "\n",
    "\n",
    "# JSON reading\n",
    "\n",
    "try:\n",
    "    reader = spark.read.schema(schema)\n",
    "    if use_multiline:\n",
    "        reader = reader.option(\"multiLine\", \"true\")\n",
    "    df_raw = reader.json(input_path)\n",
    "except Exception as e:\n",
    "    logging.error(\"Falha ao ler JSON:\\n\" + traceback.format_exc())\n",
    "    dbutils.notebook.exit(f\"ERRO ao ler JSON: {e}\")\n",
    "\n",
    "\n",
    "# Normalization / Cleaning\n",
    "\n",
    "df = (\n",
    "    df_raw\n",
    "      .withColumn(\"creator_id\", F.trim(\"creator_id\"))\n",
    "      .withColumn(\"title\",      F.trim(\"title\"))\n",
    "      .withColumn(\"yt_user\",    F.lower(F.trim(\"yt_user\")))\n",
    ")\n",
    "\n",
    "# Robust conversion of published_at:\n",
    "# - if numeric: detect epoch in ms (>= 13 digits) and divide by 1000\n",
    "# - if ISO-8601 (string with '-' and ':'): use to_timestamp directly\n",
    "df = df.withColumn(\n",
    "    \"published_at_ts\",\n",
    "    F.when(\n",
    "        F.col(\"published_at\").rlike(r\"^\\d+$\"),\n",
    "        F.to_timestamp(F.from_unixtime(\n",
    "            F.when(F.length(\"published_at\") >= 13,\n",
    "                   (F.col(\"published_at\").cast(\"double\")/1000.0)\n",
    "            ).otherwise(F.col(\"published_at\").cast(\"double\"))\n",
    "        ))\n",
    "    ).otherwise(F.to_timestamp(\"published_at\"))\n",
    ")\n",
    "\n",
    "# sanity filters\n",
    "\n",
    "df = df.filter((F.col(\"views\") >= 0) & (F.col(\"likes\") >= 0))\n",
    "\n",
    "\n",
    "# extra information (useful for monthly analysis)\n",
    "\n",
    "df = df.withColumn(\"published_month\", F.to_date(F.date_trunc(\"month\", F.col(\"published_at_ts\"))))\n",
    "\n",
    "\n",
    "# basic deduplication \n",
    "\n",
    "df = df.dropDuplicates([\"creator_id\", \"title\", \"published_at\"])\n",
    "\n",
    "\n",
    "# empty check without isEmpty()\n",
    "\n",
    "if df.limit(1).count() == 0:\n",
    "    dbutils.notebook.exit(\"ERRO: nenhum registro válido após limpeza/conversão.\")\n",
    "\n",
    "\n",
    "# log of null timestamps (debugging bad data)\n",
    "\n",
    "null_ts = df.filter(F.col(\"published_at_ts\").isNull()).count()\n",
    "if null_ts > 0:\n",
    "    logging.warning(f\"{null_ts} registro(s) com timestamp nulo em 'published_at_ts'.\")\n",
    "\n",
    "\n",
    "# Write to Delta\n",
    "\n",
    "try:\n",
    "    (df.write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .option(\"overwriteSchema\", \"true\")\n",
    "        .saveAsTable(output_table_name))\n",
    "\n",
    "    spark.sql(f\"\"\"\n",
    "      COMMENT ON TABLE {output_table_name}\n",
    "      IS 'Tabela de posts dos creators (views/likes/title/published_at_ts/tags/yt_user) — origem: {input_path}'\n",
    "    \"\"\")\n",
    "    spark.sql(f\"COMMENT ON COLUMN {output_table_name}.creator_id        IS 'Identificador do criador (fonte)';\")\n",
    "    spark.sql(f\"COMMENT ON COLUMN {output_table_name}.views             IS 'Quantidade de visualizações';\")\n",
    "    spark.sql(f\"COMMENT ON COLUMN {output_table_name}.likes             IS 'Quantidade de curtidas';\")\n",
    "    spark.sql(f\"COMMENT ON COLUMN {output_table_name}.title             IS 'Título do vídeo/post';\")\n",
    "    spark.sql(f\"COMMENT ON COLUMN {output_table_name}.published_at      IS 'Valor bruto de data/hora do dataset de origem';\")\n",
    "    spark.sql(f\"COMMENT ON COLUMN {output_table_name}.published_at_ts   IS 'Timestamp normalizado do momento de publicação';\")\n",
    "    spark.sql(f\"COMMENT ON COLUMN {output_table_name}.published_month   IS 'Primeiro dia do mês referente à publicação (DATE)';\")\n",
    "    spark.sql(f\"COMMENT ON COLUMN {output_table_name}.tags              IS 'Lista de tags do post';\")\n",
    "    spark.sql(f\"COMMENT ON COLUMN {output_table_name}.yt_user           IS 'Identificador do canal no YouTube';\")\n",
    "\n",
    "    spark.sql(f\"\"\"\n",
    "      ALTER TABLE {output_table_name}\n",
    "      SET TBLPROPERTIES (\n",
    "        'pipeline.step' = 'create_table_posts_creator',\n",
    "        'source.path'   = '{input_path}'\n",
    "      )\n",
    "    \"\"\")\n",
    "\n",
    "    logging.info(f\"Tabela '{output_table_name}' criada/sobrescrita com sucesso.\")\n",
    "except Exception as e:\n",
    "    logging.error(\"Falha ao escrever Delta:\\n\" + traceback.format_exc())\n",
    "    dbutils.notebook.exit(f\"ERRO ao escrever Delta: {e}\")\n",
    "\n",
    "\n",
    "# Return\n",
    "\n",
    "final_count = spark.table(output_table_name).count()\n",
    "payload = {\n",
    "    \"status\": \"SUCESSO\",\n",
    "    \"source_path\": input_path,\n",
    "    \"target_table\": output_table_name,\n",
    "    \"records_processed\": final_count\n",
    "}\n",
    "logging.info(f\"Processo finalizado. Registros ingeridos: {final_count}\")\n",
    "dbutils.notebook.exit(json.dumps(payload, ensure_ascii=False))\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6692143713836147,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "2 - create_table_posts_creator",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
